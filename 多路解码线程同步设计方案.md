# 🎵 多路解码线程同步设计方案

## 📋 概述

本文档详细设计了Codec2 Talkie项目中多路解码线程的同步机制，确保多路音频数据能够正确解码、混音和输出，保证音频质量和实时性。

## 🎯 同步挑战分析

### 1. 当前架构限制

#### 1.1 单线程处理模式
```java
// 当前AppWorker单线程处理
public void run() {
    android.os.Process.setThreadPriority(Process.THREAD_PRIORITY_URGENT_AUDIO);
    Looper.prepare();
    // 所有音频处理都在这个线程中
    Looper.loop();
}
```

**问题：**
- 无法并发处理多路音频
- 单点故障影响所有音频流
- 缺乏线程间同步机制

#### 1.2 时序控制机制
```java
// 10ms高频处理 + 1秒缓冲延迟
private static final int PROCESS_INTERVAL_MS = 10;
private static final int GAP_TO_PLAY_MS = 1000;
```

**问题：**
- 固定时序无法适应多路音频
- 缓冲延迟影响实时性
- 缺乏动态时序调整

### 2. 多路同步技术要求

#### 2.1 时间同步要求
- **采样率同步**: 所有音频流必须使用相同的采样率(8000Hz)
- **帧同步**: 确保音频帧的时间戳对齐
- **播放同步**: 多路音频必须同步播放

#### 2.2 数据同步要求
- **顺序保证**: 确保音频帧的播放顺序
- **完整性保证**: 防止音频帧丢失或损坏
- **一致性保证**: 多路音频数据的一致性处理

## 🏗️ 多路同步架构设计

### 1. 整体架构

```
┌─────────────────────────────────────────────────────────────┐
│                多路解码线程同步架构                          │
├─────────────────────────────────────────────────────────────┤
│  AudioSyncManager (同步管理器)                              │
│  ├─ TimeSyncController (时间同步控制器)                     │
│  ├─ DataSyncController (数据同步控制器)                     │
│  └─ OutputSyncController (输出同步控制器)                   │
├─────────────────────────────────────────────────────────────┤
│  MultiChannelDecoder (多路解码器)                          │
│  ├─ ChannelDecoder1 (Codec2/Opus)                         │
│  ├─ ChannelDecoder2 (Codec2/Opus)                         │
│  └─ ChannelDecoderN (Codec2/Opus)                        │
├─────────────────────────────────────────────────────────────┤
│  AudioMixer (音频混音器)                                    │
│  ├─ FrameBuffer (帧缓冲区)                                 │
│  ├─ MixingEngine (混音引擎)                              │
│  └─ OutputBuffer (输出缓冲区)                              │
└─────────────────────────────────────────────────────────────┘
```

### 2. 核心组件设计

#### 2.1 时间同步控制器 (TimeSyncController)

```java
public class TimeSyncController {
    private static final String TAG = "TimeSyncController";
    
    // 时间同步参数
    private static final long SYNC_INTERVAL_MS = 10;           // 10ms同步间隔
    private static final long MAX_SYNC_DRIFT_MS = 5;           // 最大同步漂移5ms
    private static final long FRAME_DURATION_MS = 20;           // 音频帧持续时间20ms
    
    // 同步状态
    private volatile long masterTimestamp = 0;                 // 主时间戳
    private volatile boolean isSyncActive = false;             // 同步激活状态
    private final Object syncLock = new Object();              // 同步锁
    
    // 多路音频时间戳
    private final Map<String, Long> channelTimestamps = new ConcurrentHashMap<>();
    private final Map<String, Long> channelDrifts = new ConcurrentHashMap<>();
    
    /**
     * 初始化时间同步
     */
    public void initializeSync() {
        synchronized (syncLock) {
            masterTimestamp = System.currentTimeMillis();
            isSyncActive = true;
            Log.i(TAG, "Time sync initialized with master timestamp: " + masterTimestamp);
        }
    }
    
    /**
     * 注册音频通道时间戳
     */
    public void registerChannelTimestamp(String channelId, long timestamp) {
        synchronized (syncLock) {
            if (!isSyncActive) {
                Log.w(TAG, "Sync not active, ignoring timestamp for channel: " + channelId);
                return;
            }
            
            long currentTime = System.currentTimeMillis();
            long drift = Math.abs(currentTime - timestamp);
            
            channelTimestamps.put(channelId, timestamp);
            channelDrifts.put(channelId, drift);
            
            // 检查同步漂移
            if (drift > MAX_SYNC_DRIFT_MS) {
                Log.w(TAG, "Channel " + channelId + " has sync drift: " + drift + "ms");
                adjustChannelSync(channelId, drift);
            }
        }
    }
    
    /**
     * 调整通道同步
     */
    private void adjustChannelSync(String channelId, long drift) {
        // 实现同步调整逻辑
        Log.i(TAG, "Adjusting sync for channel " + channelId + " by " + drift + "ms");
    }
    
    /**
     * 获取同步后的时间戳
     */
    public long getSyncedTimestamp(String channelId) {
        synchronized (syncLock) {
            Long channelTimestamp = channelTimestamps.get(channelId);
            if (channelTimestamp == null) {
                return masterTimestamp;
            }
            
            Long drift = channelDrifts.get(channelId);
            if (drift != null && drift > MAX_SYNC_DRIFT_MS) {
                // 应用同步调整
                return channelTimestamp + drift;
            }
            
            return channelTimestamp;
        }
    }
    
    /**
     * 检查所有通道是否同步
     */
    public boolean areAllChannelsSynced() {
        synchronized (syncLock) {
            if (channelTimestamps.isEmpty()) {
                return false;
            }
            
            long maxDrift = channelDrifts.values().stream()
                .mapToLong(Long::longValue)
                .max()
                .orElse(0);
            
            return maxDrift <= MAX_SYNC_DRIFT_MS;
        }
    }
}
```

#### 2.2 数据同步控制器 (DataSyncController)

```java
public class DataSyncController {
    private static final String TAG = "DataSyncController";
    
    // 数据同步参数
    private static final int MAX_BUFFER_SIZE = 1024;            // 最大缓冲区大小
    private static final int SYNC_TIMEOUT_MS = 100;            // 同步超时100ms
    private static final int MAX_RETRY_COUNT = 3;               // 最大重试次数
    
    // 同步状态
    private volatile boolean isDataSyncActive = false;          // 数据同步激活状态
    private final Object dataSyncLock = new Object();          // 数据同步锁
    
    // 多路音频数据缓冲区
    private final Map<String, BlockingQueue<AudioFrame>> channelBuffers = new ConcurrentHashMap<>();
    private final Map<String, Long> channelLastDataTime = new ConcurrentHashMap<>();
    
    /**
     * 初始化数据同步
     */
    public void initializeDataSync() {
        synchronized (dataSyncLock) {
            isDataSyncActive = true;
            Log.i(TAG, "Data sync initialized");
        }
    }
    
    /**
     * 注册音频通道数据缓冲区
     */
    public void registerChannelBuffer(String channelId) {
        synchronized (dataSyncLock) {
            channelBuffers.put(channelId, new LinkedBlockingQueue<>(MAX_BUFFER_SIZE));
            channelLastDataTime.put(channelId, 0L);
            Log.i(TAG, "Registered buffer for channel: " + channelId);
        }
    }
    
    /**
     * 添加音频帧到缓冲区
     */
    public boolean addAudioFrame(String channelId, AudioFrame frame) {
        BlockingQueue<AudioFrame> buffer = channelBuffers.get(channelId);
        if (buffer == null) {
            Log.e(TAG, "No buffer found for channel: " + channelId);
            return false;
        }
        
        try {
            boolean added = buffer.offer(frame, SYNC_TIMEOUT_MS, TimeUnit.MILLISECONDS);
            if (added) {
                channelLastDataTime.put(channelId, System.currentTimeMillis());
                Log.d(TAG, "Added frame to channel " + channelId + " buffer");
            } else {
                Log.w(TAG, "Failed to add frame to channel " + channelId + " buffer (timeout)");
            }
            return added;
        } catch (InterruptedException e) {
            Log.e(TAG, "Interrupted while adding frame to channel " + channelId, e);
            Thread.currentThread().interrupt();
            return false;
        }
    }
    
    /**
     * 获取同步的音频帧
     */
    public List<AudioFrame> getSyncedFrames() {
        List<AudioFrame> syncedFrames = new ArrayList<>();
        
        synchronized (dataSyncLock) {
            if (!isDataSyncActive) {
                return syncedFrames;
            }
            
            // 检查所有通道是否有数据
            boolean allChannelsHaveData = true;
            for (String channelId : channelBuffers.keySet()) {
                BlockingQueue<AudioFrame> buffer = channelBuffers.get(channelId);
                if (buffer.isEmpty()) {
                    allChannelsHaveData = false;
                    break;
                }
            }
            
            if (allChannelsHaveData) {
                // 从所有通道获取同步帧
                for (String channelId : channelBuffers.keySet()) {
                    BlockingQueue<AudioFrame> buffer = channelBuffers.get(channelId);
                    AudioFrame frame = buffer.poll();
                    if (frame != null) {
                        syncedFrames.add(frame);
                        Log.d(TAG, "Retrieved synced frame from channel: " + channelId);
                    }
                }
            }
        }
        
        return syncedFrames;
    }
    
    /**
     * 检查数据同步状态
     */
    public boolean isDataSynced() {
        synchronized (dataSyncLock) {
            if (channelBuffers.isEmpty()) {
                return false;
            }
            
            long currentTime = System.currentTimeMillis();
            for (String channelId : channelLastDataTime.keySet()) {
                Long lastDataTime = channelLastDataTime.get(channelId);
                if (lastDataTime == null || (currentTime - lastDataTime) > SYNC_TIMEOUT_MS) {
                    Log.w(TAG, "Channel " + channelId + " data sync timeout");
                    return false;
                }
            }
            
            return true;
        }
    }
}
```

#### 2.3 输出同步控制器 (OutputSyncController)

```java
public class OutputSyncController {
    private static final String TAG = "OutputSyncController";
    
    // 输出同步参数
    private static final int OUTPUT_BUFFER_SIZE = 2048;         // 输出缓冲区大小
    private static final int MIXING_INTERVAL_MS = 10;           // 混音间隔10ms
    private static final int MAX_OUTPUT_LATENCY_MS = 50;       // 最大输出延迟50ms
    
    // 同步状态
    private volatile boolean isOutputSyncActive = false;        // 输出同步激活状态
    private final Object outputSyncLock = new Object();       // 输出同步锁
    
    // 混音输出缓冲区
    private final AudioMixer audioMixer;
    private final AudioTrack outputAudioTrack;
    private final Timer outputTimer;
    
    public OutputSyncController(AudioMixer mixer, AudioTrack audioTrack) {
        this.audioMixer = mixer;
        this.outputAudioTrack = audioTrack;
        this.outputTimer = new Timer("OutputSyncTimer");
    }
    
    /**
     * 初始化输出同步
     */
    public void initializeOutputSync() {
        synchronized (outputSyncLock) {
            isOutputSyncActive = true;
            
            // 启动输出定时器
            outputTimer.scheduleAtFixedRate(new TimerTask() {
                @Override
                public void run() {
                    processOutputSync();
                }
            }, 0, MIXING_INTERVAL_MS);
            
            Log.i(TAG, "Output sync initialized");
        }
    }
    
    /**
     * 处理输出同步
     */
    private void processOutputSync() {
        if (!isOutputSyncActive) {
            return;
        }
        
        try {
            // 获取混音后的音频数据
            short[] mixedAudio = audioMixer.getMixedAudio();
            if (mixedAudio != null && mixedAudio.length > 0) {
                // 写入AudioTrack
                int bytesWritten = outputAudioTrack.write(mixedAudio, 0, mixedAudio.length);
                if (bytesWritten > 0) {
                    Log.d(TAG, "Wrote " + bytesWritten + " bytes to AudioTrack");
                } else {
                    Log.w(TAG, "Failed to write audio to AudioTrack");
                }
            }
        } catch (Exception e) {
            Log.e(TAG, "Error in output sync processing", e);
        }
    }
    
    /**
     * 停止输出同步
     */
    public void stopOutputSync() {
        synchronized (outputSyncLock) {
            isOutputSyncActive = false;
            outputTimer.cancel();
            Log.i(TAG, "Output sync stopped");
        }
    }
}
```

### 3. 多路解码器同步实现

#### 3.1 同步解码器基类

```java
public abstract class SyncChannelDecoder {
    private static final String TAG = "SyncChannelDecoder";
    
    protected final String channelId;
    protected final TimeSyncController timeSyncController;
    protected final DataSyncController dataSyncController;
    protected final OutputSyncController outputSyncController;
    
    // 解码器状态
    protected volatile boolean isDecoding = false;
    protected volatile long lastDecodeTime = 0;
    protected final Object decodeLock = new Object();
    
    public SyncChannelDecoder(String channelId, 
                             TimeSyncController timeSync,
                             DataSyncController dataSync,
                             OutputSyncController outputSync) {
        this.channelId = channelId;
        this.timeSyncController = timeSync;
        this.dataSyncController = dataSync;
        this.outputSyncController = outputSync;
    }
    
    /**
     * 同步解码音频帧
     */
    public void syncDecodeAudioFrame(byte[] encodedFrame) {
        synchronized (decodeLock) {
            if (!isDecoding) {
                Log.w(TAG, "Decoder not active for channel: " + channelId);
                return;
            }
            
            try {
                // 获取当前时间戳
                long currentTimestamp = System.currentTimeMillis();
                
                // 注册时间戳到同步控制器
                timeSyncController.registerChannelTimestamp(channelId, currentTimestamp);
                
                // 执行解码
                short[] decodedAudio = performDecode(encodedFrame);
                
                if (decodedAudio != null && decodedAudio.length > 0) {
                    // 创建音频帧
                    AudioFrame audioFrame = new AudioFrame(
                        channelId,
                        decodedAudio,
                        currentTimestamp,
                        decodedAudio.length
                    );
                    
                    // 添加到数据同步控制器
                    boolean added = dataSyncController.addAudioFrame(channelId, audioFrame);
                    if (added) {
                        lastDecodeTime = currentTimestamp;
                        Log.d(TAG, "Successfully decoded and synced frame for channel: " + channelId);
                    } else {
                        Log.w(TAG, "Failed to add decoded frame to sync controller for channel: " + channelId);
                    }
                } else {
                    Log.w(TAG, "Decode returned null or empty audio for channel: " + channelId);
                }
                
            } catch (Exception e) {
                Log.e(TAG, "Error in sync decode for channel: " + channelId, e);
            }
        }
    }
    
    /**
     * 抽象解码方法，由子类实现
     */
    protected abstract short[] performDecode(byte[] encodedFrame);
    
    /**
     * 启动解码器
     */
    public void startDecoding() {
        synchronized (decodeLock) {
            isDecoding = true;
            Log.i(TAG, "Started decoding for channel: " + channelId);
        }
    }
    
    /**
     * 停止解码器
     */
    public void stopDecoding() {
        synchronized (decodeLock) {
            isDecoding = false;
            Log.i(TAG, "Stopped decoding for channel: " + channelId);
        }
    }
}
```

#### 3.2 Codec2同步解码器

```java
public class SyncCodec2Decoder extends SyncChannelDecoder {
    private static final String TAG = "SyncCodec2Decoder";
    
    private long codec2Context;
    private short[] decodeBuffer;
    
    public SyncCodec2Decoder(String channelId,
                            TimeSyncController timeSync,
                            DataSyncController dataSync,
                            OutputSyncController outputSync) {
        super(channelId, timeSync, dataSync, outputSync);
        
        // 初始化Codec2解码器
        codec2Context = Codec2.create(Codec2.MODE_3200);
        decodeBuffer = new short[160]; // 20ms @ 8kHz
        
        Log.i(TAG, "Initialized Codec2 decoder for channel: " + channelId);
    }
    
    @Override
    protected short[] performDecode(byte[] encodedFrame) {
        try {
            // 调用Codec2解码
            long result = Codec2.decode(codec2Context, decodeBuffer, encodedFrame);
            
            if (result > 0) {
                // 复制解码结果
                short[] decodedAudio = new short[(int)result];
                System.arraycopy(decodeBuffer, 0, decodedAudio, 0, (int)result);
                
                Log.d(TAG, "Codec2 decoded " + result + " samples for channel: " + channelId);
                return decodedAudio;
            } else {
                Log.w(TAG, "Codec2 decode returned " + result + " for channel: " + channelId);
                return null;
            }
        } catch (Exception e) {
            Log.e(TAG, "Error in Codec2 decode for channel: " + channelId, e);
            return null;
        }
    }
    
    /**
     * 清理资源
     */
    public void cleanup() {
        if (codec2Context != 0) {
            Codec2.destroy(codec2Context);
            codec2Context = 0;
        }
        Log.i(TAG, "Cleaned up Codec2 decoder for channel: " + channelId);
    }
}
```

#### 3.3 Opus同步解码器

```java
public class SyncOpusDecoder extends SyncChannelDecoder {
    private static final String TAG = "SyncOpusDecoder";
    
    private long opusContext;
    private short[] decodeBuffer;
    private final int maxSamples;
    
    public SyncOpusDecoder(String channelId,
                          TimeSyncController timeSync,
                          DataSyncController dataSync,
                          OutputSyncController outputSync) {
        super(channelId, timeSync, dataSync, outputSync);
        
        // 初始化Opus解码器
        opusContext = Opus.create(8000, 1, Opus.OPUS_APPLICATION_VOIP, 32000, 5);
        maxSamples = 160; // 20ms @ 8kHz
        decodeBuffer = new short[maxSamples];
        
        Log.i(TAG, "Initialized Opus decoder for channel: " + channelId);
    }
    
    @Override
    protected short[] performDecode(byte[] encodedFrame) {
        try {
            // 调用Opus解码
            int decodedSamples = Opus.decode(opusContext, encodedFrame, decodeBuffer, maxSamples);
            
            if (decodedSamples > 0) {
                // 复制解码结果
                short[] decodedAudio = new short[decodedSamples];
                System.arraycopy(decodeBuffer, 0, decodedAudio, 0, decodedSamples);
                
                Log.d(TAG, "Opus decoded " + decodedSamples + " samples for channel: " + channelId);
                return decodedAudio;
            } else {
                Log.w(TAG, "Opus decode returned " + decodedSamples + " for channel: " + channelId);
                return null;
            }
        } catch (Exception e) {
            Log.e(TAG, "Error in Opus decode for channel: " + channelId, e);
            return null;
        }
    }
    
    /**
     * 清理资源
     */
    public void cleanup() {
        if (opusContext != 0) {
            Opus.destroy(opusContext);
            opusContext = 0;
        }
        Log.i(TAG, "Cleaned up Opus decoder for channel: " + channelId);
    }
}
```

### 4. 音频帧数据结构

```java
public class AudioFrame {
    private final String channelId;           // 通道ID
    private final short[] audioData;          // 音频数据
    private final long timestamp;             // 时间戳
    private final int sampleCount;            // 采样数
    private final long sequenceNumber;        // 序列号
    
    private static long globalSequenceNumber = 0;
    
    public AudioFrame(String channelId, short[] audioData, long timestamp, int sampleCount) {
        this.channelId = channelId;
        this.audioData = audioData.clone(); // 深拷贝
        this.timestamp = timestamp;
        this.sampleCount = sampleCount;
        this.sequenceNumber = ++globalSequenceNumber;
    }
    
    // Getters
    public String getChannelId() { return channelId; }
    public short[] getAudioData() { return audioData.clone(); }
    public long getTimestamp() { return timestamp; }
    public int getSampleCount() { return sampleCount; }
    public long getSequenceNumber() { return sequenceNumber; }
    
    @Override
    public String toString() {
        return String.format("AudioFrame[channel=%s, samples=%d, timestamp=%d, seq=%d]",
                channelId, sampleCount, timestamp, sequenceNumber);
    }
}
```

## 🔄 同步流程时序图

```
时间轴: 0ms    10ms   20ms   30ms   40ms   50ms
        |       |      |      |      |      |
通道1:  [帧1]   [帧2]  [帧3]  [帧4]  [帧5]  [帧6]
        |       |      |      |      |      |
通道2:  [帧1]   [帧2]  [帧3]  [帧4]  [帧5]  [帧6]
        |       |      |      |      |      |
时间同步: 注册   注册   注册   注册   注册   注册
        |       |      |      |      |      |
数据同步: 缓冲   缓冲   缓冲   缓冲   缓冲   缓冲
        |       |      |      |      |      |
混音输出: 等待   等待   混音   混音   混音   混音
        |       |      |      |      |      |
AudioTrack: 静音  静音   播放   播放   播放   播放
```

## ⚡ 性能优化策略

### 1. 内存管理优化

```java
public class AudioBufferPool {
    private static final int POOL_SIZE = 10;
    private final Queue<short[]> bufferPool = new ConcurrentLinkedQueue<>();
    private final Queue<AudioFrame> framePool = new ConcurrentLinkedQueue<>();
    
    public AudioBufferPool() {
        // 预分配缓冲区
        for (int i = 0; i < POOL_SIZE; i++) {
            bufferPool.offer(new short[160]); // 20ms @ 8kHz
            framePool.offer(new AudioFrame("", new short[0], 0, 0));
        }
    }
    
    public short[] getBuffer() {
        short[] buffer = bufferPool.poll();
        return buffer != null ? buffer : new short[160];
    }
    
    public void returnBuffer(short[] buffer) {
        if (buffer.length == 160 && bufferPool.size() < POOL_SIZE) {
            Arrays.fill(buffer, (short)0); // 清零
            bufferPool.offer(buffer);
        }
    }
}
```

### 2. 线程优先级优化

```java
public class SyncThreadManager {
    private static final int AUDIO_THREAD_PRIORITY = Process.THREAD_PRIORITY_URGENT_AUDIO;
    private static final int HIGH_THREAD_PRIORITY = Process.THREAD_PRIORITY_URGENT_DISPLAY;
    
    public static void setAudioThreadPriority(Thread thread) {
        android.os.Process.setThreadPriority(AUDIO_THREAD_PRIORITY);
    }
    
    public static void setHighThreadPriority(Thread thread) {
        android.os.Process.setThreadPriority(HIGH_THREAD_PRIORITY);
    }
}
```

### 3. 同步性能监控

```java
public class SyncPerformanceMonitor {
    private final Map<String, Long> channelLatencies = new ConcurrentHashMap<>();
    private final Map<String, Integer> channelDropCounts = new ConcurrentHashMap<>();
    
    public void recordChannelLatency(String channelId, long latency) {
        channelLatencies.put(channelId, latency);
    }
    
    public void recordChannelDrop(String channelId) {
        channelDropCounts.merge(channelId, 1, Integer::sum);
    }
    
    public void logPerformanceStats() {
        Log.i("SyncPerformance", "Channel latencies: " + channelLatencies);
        Log.i("SyncPerformance", "Channel drops: " + channelDropCounts);
    }
}
```

## 🛡️ 错误处理和恢复

### 1. 同步错误处理

```java
public class SyncErrorHandler {
    private static final int MAX_SYNC_ERRORS = 5;
    private final Map<String, Integer> channelErrorCounts = new ConcurrentHashMap<>();
    
    public boolean handleSyncError(String channelId, Exception error) {
        int errorCount = channelErrorCounts.merge(channelId, 1, Integer::sum);
        
        if (errorCount > MAX_SYNC_ERRORS) {
            Log.e("SyncError", "Too many sync errors for channel " + channelId + ", disabling");
            return false; // 禁用通道
        }
        
        Log.w("SyncError", "Sync error for channel " + channelId + ": " + error.getMessage());
        return true; // 继续处理
    }
    
    public void resetChannelErrors(String channelId) {
        channelErrorCounts.remove(channelId);
    }
}
```

### 2. 自动恢复机制

```java
public class SyncRecoveryManager {
    private static final long RECOVERY_CHECK_INTERVAL_MS = 1000;
    private final Timer recoveryTimer;
    
    public SyncRecoveryManager() {
        this.recoveryTimer = new Timer("SyncRecoveryTimer");
    }
    
    public void startRecoveryMonitoring() {
        recoveryTimer.scheduleAtFixedRate(new TimerTask() {
            @Override
            public void run() {
                checkAndRecoverSync();
            }
        }, RECOVERY_CHECK_INTERVAL_MS, RECOVERY_CHECK_INTERVAL_MS);
    }
    
    private void checkAndRecoverSync() {
        // 检查同步状态并执行恢复操作
        Log.d("SyncRecovery", "Checking sync recovery status");
    }
}
```

## 📊 总结

本设计方案提供了完整的多路解码线程同步解决方案：

1. **时间同步**: 通过TimeSyncController确保多路音频的时间戳对齐
2. **数据同步**: 通过DataSyncController保证音频帧的完整性和顺序
3. **输出同步**: 通过OutputSyncController确保混音输出的实时性
4. **性能优化**: 内存池、线程优先级、性能监控等优化措施
5. **错误处理**: 完善的错误处理和自动恢复机制

该方案能够确保多路解码线程的同步，保证音频质量和实时性，为Codec2 Talkie项目提供可靠的多路音频处理能力。
